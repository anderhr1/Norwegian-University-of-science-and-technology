Before commencing any empirical work, it is essential to thoroughly review the existing literature, and the relevant articles that are found can be summarized in the literature review section. This will not only help to put the proposed research in a relevant context, but also may highlight potential problem areas, and will ensure up-to-date techniques are used and that the project is not a direct (even unintentional) copy of an already existing work. The literature review should follow the style of an extended literature reviews in a scholarly journal, and should always be CRITICAL by nature. It should comment on the relevance, value, advantages and shortcomings of the cited articles. Finally you PLACE your research into the body of literature.


\chapter{Literature Review}

\textbf{1. What is IV}
\begin{itemize}
    \item One potential explanation for the latter result is that the idiosyncratic volatility is a measure of divergence of opinion, which, as argued by Miller (1977), could lead a stock to be overvalued initially and to suffer capital losses subsequently.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.5]{Plot/DummyLITREW.png}
    \caption{Regression: average daily IV percentage on annualized economic standard deviation}
    \label{IVtoVol}
\end{figure}

In this chapter we will review the literature in three sections. The two first sections will review what can be perceived as independent literature, while the third section will try to connect the first two sections and link them to the Norwegian stock market. 

\section*{Idiosyncratic volatility}

The behavior of stock returns, mainly in older and more consolidated stock markets such as those in the USA, has been studied for a long time.

The well-known Capital Asset Pricing Model (CAPM) proposed by Sharpe (1964) and Lintner (1965), is widely used to determine an asset's theoretical returns. Modern finance affirms that investors hold diversified stock portfolios to reduce idiosyncratic risk, which is a stock's specific risk. According to the CAPM, all investors should have a balanced market portfolio to eliminate all of the stock market's idiosyncratic risk. However, in practice, neither individual nor institutional investors hold such diversified portfolios and thus, some idiosyncratic risk is priced into their portfolios (Fu, 2009).

Various studies have attempted to complement the CAPM or to question its validity.Banz (1981) was an important work that analyzed the relation between returns and firms' market values. The author discovered the so-called size effect in stocks traded on the New York Stock Exchange, in which the performance of the stocks of smaller firms is superior to that of larger firms. According to Banz (1981), the size effect represents a failure of the CAPM specification because, for a specific beta, the average return of a stock with a lower market value is superior to that of a stock with a higher market value.

In addition, Banz (1981) served as the basis for other important and fundamental studies: Fama and French (1992), followed by Fama and French (1993), which developed their Three-Factor Model. Fama and French (1993) investigated the main risk factors associated with stock returns. Their model uses the following factors: market returns; the returns of a small minus big (SMB) variable, calculated as the average returns of small firm stock portfolios minus the average returns of large firm stock portfolios; and a high minus low (HML) variable, calculated as the difference in the returns of portfolios formed by firms with high and low book-to-market ratios. Fama and French (1993) conclude that the Three-Factor Model is superior to the CAPM in explaining average returns and that the model's three coefficients are simultaneously significant.

Various theories assume that idiosyncratic risk is positively correlated with stocks' expected returns. The idea behind this assumption is that investors who do not diversify their investments demand an additional return in order to bear the risk of their portfolios. The main exponents of these theories are Levy (1978), Merton (1973), and Malkiel and Xu (2002).

The empirical existence of a relationship between idiosyncratic risk and expected returns has been tested for a considerable amount of time. However, as highlighted in Fu and Schutte (2010), articles that find a positive relationship between these variables are almost equal in number to those that find no relation, or even a negative one. For example, Goyal and Santa-Clara (2003), who found evidence that market variance does not predict returns, should be highlighted. However, they found a positive and significant relationship between average stock variance, whose greatest component is idiosyncratic risk, and market returns. Goyal and Santa-Clara (2003) used a portfolio of stocks traded on the New York Stock Exchange (NYSE), American Stock Exchange (AMEX) and Nasdaq exchanges between August 1963 and December 1999.

Malkiel and Xu (2002) also found a positive relationship between idiosyncratic volatility and the cross-section of expected returns using the tests developed in Fama and Macbeth (1973) and Fama and French (1992). Malkiel and Xu (2002) arrived at the conclusion that idiosyncratic risk is more important than firm size, or beta, in explaining the cross-section of returns. Factors such as firm size, book-to-market ratio and liquidity were used as control variables in the cross-section regressions. The data covered stocks traded on the NYSE, AMEX and Nasdaq exchanges, as well as stocks traded on the Tokyo Stock Exchange (TSE), during the period from 1975 to 2000.

Kotiaho (2010) performed a similar study using stocks traded on the NYSE, AMEX and Nasdaq exchanges during the period from 1971 to 2008 and found a positive relation between stocks' idiosyncratic risks and expected returns, which was mainly due to the behavior of small-company stocks.

In contrast, other authors found no relation, or \textbf{even a negative one, between stocks' specific risk components and expected returns.}

Ang, Hodrick, Xing, and Zhang (2009) used data from 23 countries and concluded that high idiosyncratic volatility stocks generate lower future returns than low idiosyncratic volatility stocks. A previous article (Ang, Hodrick, Xing, & Zhang, 2006) showed a negative relation between a stock's monthly returns and its 1-month lagged idiosyncratic risk.

However, these conclusions are contested in Fu (2009), who contends that Ang et al.'s (2009) result was influenced by the stocks of smaller high idiosyncratic volatility firms. Fu (2009) replicated the method used in Ang et al. (2006) to estimate idiosyncratic volatility. The statistics of the series show, however, that idiosyncratic risk varies over time and its 1-month lagged value is therefore not a good proxy for the current month's expected risk. Thus, Fu (2009) proposes the use of the EGARCH model to estimate expected idiosyncratic volatility and this variable is included in the cross-section regressions along with other explanatory variables. The data covered the period from July 1963 to December 2006 for stocks traded on the NYSE, AMEX and Nasdaq exchanges. The results show a positive and statistically significant relation.

Bali and Cakici (2006) found no relation between an \textbf{equally weighted} stock portfolio's returns and its idiosyncratic risk Huang, Liu, Rhee, and Zhang (2010) contest these, as well as Ang et al.'s (2006) results. Huang et al.'s (2010) analysis shows that, in both cases, the obtained relation can be explained by short-term mean-reversion.

Angelidis (2010) investigates volatility's idiosyncratic component in 24 emerging countries. The study confirms the idea that the percentage of volatility that can be attributed to an asset's specific risk is lower in emerging markets than in developed markets, given the latter's greater efficiency. Angelidis (2010) also tested the relation between idiosyncratic risk and returns in these countries and the results show that idiosyncratic risk is a predictor of returns only when considered along with market risk.

In the case of the Norwegian market, some studies have already analyzed idiosyncratic risk.

\section*{Forecasting models}

Financial time series prediction deals with the task of modelling the underlying data generation process using past observations and using the model to extrapolate the time series into the future. Due to its intrinsic difficulty and widespread applications, much effort was devoted in the past few decades to the development and refining of financial forecasting models. In the literature, there are some classical methods that have been developed in predicting financial time series. Among them, there is a powerful method, available in the literature for univariate time-series forecasting, known as Box and Jenkins [4] ARMA approach on stationary time series. In conventional econometric models, the variance of the disturbance is assumed to be constant. But many economic and financial time series such as exchange rates, stock market indices, market returns, inflation rate, etc. exhibit periods of unusual large volatility, followed by periods of relative tranquillity (i.e. time series exhibits clustering of large and small disturbances). 

Such circumstances suggest a form of heteroskedasticity in which the variance of the disturbance depends on the size of preceding disturbance and hence the conditional variance is non-constant over the sample period. Engle [13] showed that it is possible to model the mean and the variance of a series simultaneously by a model named autoregressive conditional heteroskedastic (ARCH); whereas Box and Jenkins [4] modeled only the mean series. In latter, Bollerslev [3] extended Engleâ€™s original work by developing a technique that allows the conditional variance to be an ARMA process and that extended process is known as the GARCH process. To deal with the intricacy specially, Wong et al. [48] adopted the well-known GARCH model in the form of the so-called mixture of AR-GARCH model in exchange rate prediction. Again, Tang et al. [39] explored the mixture of ARMA-GARCH model for stock price prediction. These time series models capture several important features of financial series, such as leptokurticity and volatility clustering. . Further empirical comparisons between the mixture of AR and mixture of ARMA models for predicting financial exchange data were also conducted (Kwok et al., 1998). Recently, a theoretical study on the mixture of AR model was shown in (Wong et al., 2000), which highlights its advantages over conventional AR model. Evidence on the forecasting ability of the GARCH model is somewhat mixed. Anderson and Bollerslev [1] showed that the GARCH model provides good volatility forecast. Conversely, some empirical studies showed that the GARCH model tends to give poor forecasting performances [5,9,14,20,21,29]. 

To improve the forecasting ability of the GARCH model, some alternative approaches have been advocated from the perspective of estimation and forecasting. Neural network (NN) is one such method. The classical methods are based on some specific assumptions, such as linearity; or on error distributions, such as normality. In recent years, NNs have been successfully used for forecasting financial time series. The reason for its popularity in finance can be attributed to the ability to approximate any nonlinear relationship with a reasonable degree of accuracy. However, NN suffers from a number of weaknesses including the need for a large number of controlling parameters, difficulty in obtaining a global solution and the danger of over-fitting.

Recently, a novel Artificial Intelligence (AI) algorithm, called support-vector machine (SVM), was developed by Vapnik [44,45] and since then it has been gaining popularity due to many attractive features. While the traditional NN implements the empirical risk minimization (ERM) principle, SVM implements the structural risk minimization (SRM) principle which seeks to minimize the upper bound of the population risk using the concept of the Vapnikâ€“Chervonenkis dimension, as opposed to ERM, that minimizes the error on the in-sample estimating data. Based on the SRM principle, SVM achieves a balance between the training error and generalization error, leading to better forecasting performance than traditional NN. Selecting the best model in SVM is equivalent to solving a quadratic programming, which gives SVM another merit of a unique global solution. 

Many researchers have already published huge number of papers comparing autoregressive (AR) orARMA or GARCH models with NN and SVM in financial time-series forecasting. There are some recent papers in this combination. For example, Chen et al. [7] compared SVMs and BPs taking AR as a benchmark in forecasting the six major Asian stock markets, and Chen et al. [6] proposed recurrent SVR based on the GARCH model compared with moving average (MA), recurrent NN and parametric GARCH model in terms of their ability to forecast volatility. But they did not take account of the finite mixture ARMA-GARCH model in this combination. However, Santos et al. [35] compared ARMA-GARCH model with ARMA, ANNs and fuzzy systems in forecasting exchange rates, but they did not consider SVM in this combination. 


\section*{Relationship between idiosyncratic volatility and return forecasting performance in the Norwegian stock market}

In relation to studies that focus on the Norwegian stock market